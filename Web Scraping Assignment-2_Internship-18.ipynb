{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required packages\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the required webpage\n",
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering “Data Analyst” in “Skill,Designations,Companies” field\n",
    "driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Analyst\") #job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering “Bangalore” in “enter the location” field.\n",
    "driver.find_element_by_id(\"qsb-location-sugg\").send_keys(\"Bangalore\") #location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath(\"//button[@class='btn']\").click() #Clicking the search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists for scraping data\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the data using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the job titles\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    job_title.append(i.text)\n",
    "\n",
    "#Scraping the job locations\n",
    "for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span[1]\"):\n",
    "    job_location.append(i.text)\n",
    "    \n",
    "#Scraping the company names\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\"):\n",
    "    company_name.append(i.text)\n",
    "    \n",
    "#Scraping the experience required\n",
    "for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span[1]\"):\n",
    "    experience.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Analyst - SQL/Tableau/Redshift</td>\n",
       "      <td>Noida, Mumbai, Indore, Hyderabad/Secunderabad,...</td>\n",
       "      <td>Pronto Consulting Services</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hiring Business data analyst with Strong SQL S...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Hexaware Technologies Ltd.</td>\n",
       "      <td>9-13 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Publicis Groupe</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst - Work from Home</td>\n",
       "      <td>Pune, Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "      <td>POWER STORES E COMMERCE PRIVATE LIMITED</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Innovsource Services Private Limited</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sr. Data Analyst DW BI- Immediate Joiners</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Kelly Outsourcing and Consulting Group India P...</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Business Analyst - Data Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>BankBazaar</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(2nd Phase JP Nagar)</td>\n",
       "      <td>Liventus, Inc.</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Safety Data Analyst - BI</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Novo Nordisk India</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0         Senior Data Analyst - SQL/Tableau/Redshift   \n",
       "1  Hiring Business data analyst with Strong SQL S...   \n",
       "2                                Senior Data Analyst   \n",
       "3                      Data Analyst - Work from Home   \n",
       "4                                       Data Analyst   \n",
       "5          Sr. Data Analyst DW BI- Immediate Joiners   \n",
       "6                  Business Analyst - Data Analytics   \n",
       "7                                Senior Data Analyst   \n",
       "8                    Senior Safety Data Analyst - BI   \n",
       "9                              Business Data Analyst   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Noida, Mumbai, Indore, Hyderabad/Secunderabad,...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3      Pune, Bangalore/Bengaluru, Mumbai (All Areas)   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                             Remote   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7            Bangalore/Bengaluru(2nd Phase JP Nagar)   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                             Company Experience Required  \n",
       "0                         Pronto Consulting Services            6-10 Yrs  \n",
       "1                         Hexaware Technologies Ltd.            9-13 Yrs  \n",
       "2                                    Publicis Groupe             1-5 Yrs  \n",
       "3            POWER STORES E COMMERCE PRIVATE LIMITED             2-5 Yrs  \n",
       "4               Innovsource Services Private Limited             1-6 Yrs  \n",
       "5  Kelly Outsourcing and Consulting Group India P...            5-10 Yrs  \n",
       "6                                         BankBazaar             1-5 Yrs  \n",
       "7                                     Liventus, Inc.             5-8 Yrs  \n",
       "8                                 Novo Nordisk India            5-10 Yrs  \n",
       "9                                             NetApp             3-6 Yrs  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "jobs=pd.DataFrame({})\n",
    "jobs[\"Job Title\"]=job_title[:10]\n",
    "jobs[\"Location\"]=job_location[:10]\n",
    "jobs[\"Company\"]=company_name[:10]\n",
    "jobs[\"Experience Required\"]=experience[:10]\n",
    "#printing dataframe\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the required webpage\n",
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering \"Data Scientist\" in “Skill,Designations,Companies” field\n",
    "driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Scientist\") #job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering \"Bangalore\" in “enter the location” field.\n",
    "driver.find_element_by_id(\"qsb-location-sugg\").send_keys(\"Bangalore\") #location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath(\"//button[@class='btn']\").click() #Cliaking on the search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list for scraping data\n",
    "urls=[]\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "job_desc=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the data using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the job titles\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "#Scraping the job locations\n",
    "for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span[1]\"):\n",
    "    job_location.append(i.text)\n",
    "    \n",
    "#Scraping the company names\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\"):\n",
    "    company_name.append(i.text)\n",
    "    \n",
    "#Scraping the job urls to scrape Full Description \n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the full description by running for loop over the scraped urls\n",
    "for i in urls[:10]:\n",
    "    driver.get(i)\n",
    "    job_desc.append(driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\").text.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Full Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>RANDSTAD INDIA PVT LTD</td>\n",
       "      <td>Roles and ResponsibilitiesJob DescriptionWhat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineers , Data Scientist and SAP</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Teamware Solutions</td>\n",
       "      <td>Dynamic recruiting professional with 7 - 10 ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Software Engineer - Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Ara Resources</td>\n",
       "      <td>Key Responsibilities:• Harvest Natural Languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Associate - Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Novotree Minds Consulting Pvt Limited</td>\n",
       "      <td>Roles and ResponsibilitiesAs a Senior Associat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sr . Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Perform Group</td>\n",
       "      <td>Required Skills:Prior experience developing de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Gojek Tech</td>\n",
       "      <td>What You Will DoBuild models (or ML models) th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>GO-JEK India</td>\n",
       "      <td>What You Will DoBuild models (or ML models) th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>Kaleidofin</td>\n",
       "      <td>Key Responsibilities:You will work with the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sr Data Scientist (Level 10)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Siemens Limited</td>\n",
       "      <td>This is your role. What part will you playMin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sr Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>FabHotel Aay Kay Model Town</td>\n",
       "      <td>As as Senior Data Scientist, you will work in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Job Title                      Location  \\\n",
       "0                      Senior Data Scientist           Bangalore/Bengaluru   \n",
       "1    Data Engineers , Data Scientist and SAP           Bangalore/Bengaluru   \n",
       "2  Senior Software Engineer - Data Scientist           Bangalore/Bengaluru   \n",
       "3          Senior Associate - Data Scientist           Bangalore/Bengaluru   \n",
       "4                        Sr . Data Scientist           Bangalore/Bengaluru   \n",
       "5                      Senior Data Scientist           Bangalore/Bengaluru   \n",
       "6                      Senior Data Scientist           Bangalore/Bengaluru   \n",
       "7                      Senior Data Scientist  Chennai, Bangalore/Bengaluru   \n",
       "8               Sr Data Scientist (Level 10)           Bangalore/Bengaluru   \n",
       "9                          Sr Data Scientist           Bangalore/Bengaluru   \n",
       "\n",
       "                                 Company  \\\n",
       "0                 RANDSTAD INDIA PVT LTD   \n",
       "1                     Teamware Solutions   \n",
       "2                          Ara Resources   \n",
       "3  Novotree Minds Consulting Pvt Limited   \n",
       "4                          Perform Group   \n",
       "5                             Gojek Tech   \n",
       "6                           GO-JEK India   \n",
       "7                             Kaleidofin   \n",
       "8                        Siemens Limited   \n",
       "9            FabHotel Aay Kay Model Town   \n",
       "\n",
       "                                Full Job Description  \n",
       "0  Roles and ResponsibilitiesJob DescriptionWhat ...  \n",
       "1  Dynamic recruiting professional with 7 - 10 ye...  \n",
       "2  Key Responsibilities:• Harvest Natural Languag...  \n",
       "3  Roles and ResponsibilitiesAs a Senior Associat...  \n",
       "4  Required Skills:Prior experience developing de...  \n",
       "5  What You Will DoBuild models (or ML models) th...  \n",
       "6  What You Will DoBuild models (or ML models) th...  \n",
       "7  Key Responsibilities:You will work with the fo...  \n",
       "8  This is your role. What part will you playMin ...  \n",
       "9  As as Senior Data Scientist, you will work in ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "jobs=pd.DataFrame({})\n",
    "jobs[\"Job Title\"]=job_title[:10]\n",
    "jobs[\"Location\"]=job_location[:10]\n",
    "jobs[\"Company\"]=company_name[:10]\n",
    "jobs[\"Full Job Description\"]=job_desc\n",
    "#printing dataframe\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: In this question you have to scrape data using the filters available on the webpage. You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "=>The location filter to be used is “Delhi/NCR”\n",
    "=>The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the required webpage\n",
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering \"Data Scientist\" in “Skill,Designations,Companies” field\n",
    "driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Scientist\") #job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath(\"//button[@class='btn']\").click() #Clicking on search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the \"Delhi/NCR\" filter checkbox\n",
    "driver.find_element_by_xpath(\"//label[@for='chk-Delhi / NCR-cityTypeGid-']//i\").click()\n",
    "\n",
    "time.sleep(4) #waiting for 4 seconds\n",
    "\n",
    "#Checking the \"3-6 lakhs\" filter checkbox\n",
    "driver.find_element_by_xpath(\"//label[@for='chk-3-6 Lakhs-ctcFilter-']//i\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists for scraping data\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the data using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the job titles\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "#Scraping the job locations\n",
    "for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span[1]\"):\n",
    "    job_location.append(i.text)\n",
    "    \n",
    "#Scraping the company names\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\"):\n",
    "    company_name.append(i.text)\n",
    "    \n",
    "#Scraping the experience required\n",
    "for i in driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span[1]\"):\n",
    "    experience.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist - Noida</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist / Data Analyst</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>CARS24</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Machine Learning/NLP</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>TalPro</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - Machine Learning/NLP</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>TalPro</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring Data Scientist Develope || IDS Infotech...</td>\n",
       "      <td>Chandigarh, Hyderabad/Secunderabad, Chennai, B...</td>\n",
       "      <td>IDS Infotech Ltd.</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hiring Data Scientist Develope || IDS Infotech...</td>\n",
       "      <td>Chandigarh, Hyderabad/Secunderabad, Chennai, B...</td>\n",
       "      <td>IDS Infotech Ltd.</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...</td>\n",
       "      <td>inVentiv International Pharma Services Pvt. Ltd.</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida(Sector-59 Noida)</td>\n",
       "      <td>R Systems International Ltd.</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                      Senior Data Scientist - Noida   \n",
       "1  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "2                      Data Scientist / Data Analyst   \n",
       "3              Data Scientist - Machine Learning/NLP   \n",
       "4              Data Scientist - Machine Learning/NLP   \n",
       "5  Hiring Data Scientist Develope || IDS Infotech...   \n",
       "6  Hiring Data Scientist Develope || IDS Infotech...   \n",
       "7                              Senior Data Scientist   \n",
       "8                                     Data Scientist   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                              Noida   \n",
       "1               Noida, Gurgaon/Gurugram, Delhi / NCR   \n",
       "2                                   Gurgaon/Gurugram   \n",
       "3                                   Gurgaon/Gurugram   \n",
       "4                                   Gurgaon/Gurugram   \n",
       "5  Chandigarh, Hyderabad/Secunderabad, Chennai, B...   \n",
       "6  Chandigarh, Hyderabad/Secunderabad, Chennai, B...   \n",
       "7  Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...   \n",
       "8                                              Noida   \n",
       "9                             Noida(Sector-59 Noida)   \n",
       "\n",
       "                                            Company Experience Required  \n",
       "0    Optum Global Solutions (India) Private Limited             2-6 Yrs  \n",
       "1                         GABA Consultancy services             0-0 Yrs  \n",
       "2                                            CARS24             1-5 Yrs  \n",
       "3                                            TalPro             2-6 Yrs  \n",
       "4                                            TalPro             2-4 Yrs  \n",
       "5                                 IDS Infotech Ltd.            5-10 Yrs  \n",
       "6                                 IDS Infotech Ltd.            5-10 Yrs  \n",
       "7  inVentiv International Pharma Services Pvt. Ltd.             3-6 Yrs  \n",
       "8                Ashkom Media India Private Limited             3-8 Yrs  \n",
       "9                      R Systems International Ltd.            5-10 Yrs  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Job Title']=job_title[:10]\n",
    "jobs[\"Location\"]=job_location[:10]\n",
    "jobs[\"Company\"]=company_name[:10]\n",
    "jobs[\"Experience Required\"]=experience[:10]\n",
    "#printing dataframe\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the required webpage\n",
    "url=\"https://www.glassdoor.co.in/index.htm\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering \"Data Scientist\" in the job search bar\n",
    "driver.find_element_by_id(\"sc.keyword\").send_keys(\"Data Scientist\") #job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Keys package for the Backspace method\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    driver.find_element_by_id(\"sc.location\").send_keys(Keys.BACK_SPACE) #Clearing the \"Location\" search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering \"Noida\" in the location search bar\n",
    "driver.find_element_by_id(\"sc.location\").send_keys(\"Noida\") #location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the search button\n",
    "driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto search__SearchStyles__newSearchButton css-iixdfr']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists for scraping the data\n",
    "company_name=[]\n",
    "days=[]\n",
    "rating=[]\n",
    "urls=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the data using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping company names and its urls\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class=' css-l2wjgv e1n63ojh0 jobLink']\"):\n",
    "    company_name.append(i.text)\n",
    "    urls.append(i.get_attribute('href'))\n",
    "    \n",
    "#Scraping \"No. of days ago when the job was posted\"\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\"):\n",
    "    days.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running for loop over first 10 urls fetched\n",
    "for i in urls[:10]:\n",
    "    driver.get(i) #Getting each job page\n",
    "    \n",
    "    try:\n",
    "        r=driver.find_element_by_xpath(\"//span[@class='css-1pmc6te e11nt52q4']\")\n",
    "        rating.append(r.text.replace(\"\\n\",'')) #appending Rating if its found\n",
    "    except:\n",
    "        rating.append(\"-\") #If no Rating is found, then \"-\" is appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>No. of Days ago when Job was posted</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liberin Technologies Private Limited</td>\n",
       "      <td>7d</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Innovacer</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.8★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pixel Vision</td>\n",
       "      <td>18d</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>30d+</td>\n",
       "      <td>4.1★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td>3d</td>\n",
       "      <td>4.1★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>4d</td>\n",
       "      <td>4.4★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Newgen Software</td>\n",
       "      <td>18d</td>\n",
       "      <td>3.3★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NatWest Group</td>\n",
       "      <td>1d</td>\n",
       "      <td>3.9★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Crowe</td>\n",
       "      <td>30d+</td>\n",
       "      <td>3.8★</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Company Name No. of Days ago when Job was posted  \\\n",
       "0  Liberin Technologies Private Limited                                  7d   \n",
       "1                             Innovacer                                 24h   \n",
       "2                          Pixel Vision                                 18d   \n",
       "3                        Biz2Credit Inc                                30d+   \n",
       "4                              Ericsson                                  3d   \n",
       "5          Salasar New Age Technologies                                30d+   \n",
       "6                             Microsoft                                  4d   \n",
       "7                       Newgen Software                                 18d   \n",
       "8                         NatWest Group                                  1d   \n",
       "9                                 Crowe                                30d+   \n",
       "\n",
       "  Ratings  \n",
       "0       -  \n",
       "1    3.8★  \n",
       "2       -  \n",
       "3    4.1★  \n",
       "4    4.1★  \n",
       "5       -  \n",
       "6    4.4★  \n",
       "7    3.3★  \n",
       "8    3.9★  \n",
       "9    3.8★  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 10 companies\n",
    "jobs=pd.DataFrame({})\n",
    "jobs[\"Company Name\"]=company_name[:10]\n",
    "jobs[\"No. of Days ago when Job was posted\"]=days[:10]\n",
    "jobs[\"Ratings\"]=rating\n",
    "#printing dataframe\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location. You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary. Scrape data for first 10 companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the required webpage\n",
    "url='https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering \"Data Scientist\" in the job search bar\n",
    "driver.find_element_by_id(\"KeywordSearch\").send_keys('Data Scientist') #job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    driver.find_element_by_id(\"LocationSearch\").send_keys(Keys.BACK_SPACE) #Clearing the \"Location\" search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering \"Noida\" in the location search bar\n",
    "driver.find_element_by_id(\"LocationSearch\").send_keys(\"Noida\") #location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the search button\n",
    "driver.find_element_by_id(\"HeroSearchButton\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists for scraping data\n",
    "company_name=[]\n",
    "no_salaries=[]\n",
    "avg_salary=[]\n",
    "minsal=[]\n",
    "maxsal=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the data using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the company names\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='css-f3vw95 e1aj7ssy3']\"):\n",
    "    company_name.append(i.text)\n",
    "    \n",
    "#Scraping the Number of Salaries\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='m-0 css-1b6bxoo']\"):\n",
    "    no_salaries.append(i.text)\n",
    "\n",
    "#Scraping the average salary    \n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='col-12 col-lg-4 px-lg-0 d-flex align-items-baseline']//h3\"):\n",
    "    avg_salary.append(i.text)\n",
    "\n",
    "#Scraping the Min salary\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='d-flex mt-xxsm css-79elbk epuxyqn0']//p[1]\"):\n",
    "    minsal.append(i.text)\n",
    "\n",
    "#Scraping the Max salary\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='d-flex mt-xxsm css-79elbk epuxyqn0']//p[2]\"):\n",
    "    maxsal.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Maximum Salary</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>No of Salaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tata Consultancy Services (13461)</td>\n",
       "      <td>₹3L</td>\n",
       "      <td>₹13L</td>\n",
       "      <td>₹6,12,205</td>\n",
       "      <td>18 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IBM</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹27L</td>\n",
       "      <td>₹9,00,000</td>\n",
       "      <td>18 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹22L</td>\n",
       "      <td>₹11,63,336</td>\n",
       "      <td>15 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>₹5L</td>\n",
       "      <td>₹1Cr</td>\n",
       "      <td>₹12,18,244</td>\n",
       "      <td>15 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>₹4L</td>\n",
       "      <td>₹16L</td>\n",
       "      <td>₹7,39,238</td>\n",
       "      <td>14 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>₹8L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>₹12,80,000</td>\n",
       "      <td>14 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>₹5L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>₹8,63,750</td>\n",
       "      <td>10 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>₹11,10,000</td>\n",
       "      <td>9 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Optum</td>\n",
       "      <td>₹8L</td>\n",
       "      <td>₹20L</td>\n",
       "      <td>₹14,23,677</td>\n",
       "      <td>9 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Optum Global Solutions</td>\n",
       "      <td>₹4L</td>\n",
       "      <td>₹22L</td>\n",
       "      <td>₹13,28,697</td>\n",
       "      <td>9 salaries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Company Name Minimum Salary Maximum Salary  \\\n",
       "0  Tata Consultancy Services (13461)            ₹3L           ₹13L   \n",
       "1                                IBM            ₹6L           ₹27L   \n",
       "2                          Accenture            ₹6L           ₹22L   \n",
       "3                          Delhivery            ₹5L           ₹1Cr   \n",
       "4                 Ericsson-Worldwide            ₹4L           ₹16L   \n",
       "5                 UnitedHealth Group            ₹8L           ₹15L   \n",
       "6                 Valiance Solutions            ₹5L           ₹15L   \n",
       "7                        EXL Service            ₹6L           ₹15L   \n",
       "8                              Optum            ₹8L           ₹20L   \n",
       "9             Optum Global Solutions            ₹4L           ₹22L   \n",
       "\n",
       "  Average Salary No of Salaries  \n",
       "0      ₹6,12,205    18 salaries  \n",
       "1      ₹9,00,000    18 salaries  \n",
       "2     ₹11,63,336    15 salaries  \n",
       "3     ₹12,18,244    15 salaries  \n",
       "4      ₹7,39,238    14 salaries  \n",
       "5     ₹12,80,000    14 salaries  \n",
       "6      ₹8,63,750    10 salaries  \n",
       "7     ₹11,10,000     9 salaries  \n",
       "8     ₹14,23,677     9 salaries  \n",
       "9     ₹13,28,697     9 salaries  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 10 companies\n",
    "company=pd.DataFrame({})\n",
    "company[\"Company Name\"]=company_name[:10]\n",
    "company[\"Minimum Salary\"]=minsal[:10]\n",
    "company[\"Maximum Salary\"]=maxsal[:10]\n",
    "company[\"Average Salary\"]=avg_salary[:10]\n",
    "company[\"No of Salaries\"]=no_salaries[2:12]\n",
    "#printing dataframe\n",
    "company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the required webpage\n",
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering \"sunglasses\" in the search bar\n",
    "driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\").send_keys(\"sunglasses\") #product search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking the search button\n",
    "driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists for scraping data\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "price=[]\n",
    "discount=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping data from the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the brand names\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "    brand.append(i.text)\n",
    "\n",
    "#Scraping the product descriptions\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='IRpwTa' or @class='IRpwTa _2-ICcC']\"):\n",
    "    prod_desc.append(i.text)\n",
    "\n",
    "#Scraping the prices\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "    price.append(i.text)\n",
    "    \n",
    "#Scraping the discounts\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\"):\n",
    "    discount.append(i.text)\n",
    "\n",
    "#Clicking on Next button\n",
    "driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping data from the next two pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    #Scraping the brand names\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "        brand.append(j.text)\n",
    "    \n",
    "    #Scraping the product descriptions\n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='IRpwTa' or @class='IRpwTa _2-ICcC']\"):\n",
    "        prod_desc.append(j.text)\n",
    "    \n",
    "    #Scraping the prices\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "        price.append(j.text)\n",
    "    \n",
    "    #Scraping the discounts\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\"):\n",
    "        discount.append(j.text)\n",
    "        \n",
    "    #Clicking on next button\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]\").click()\n",
    "    time.sleep(4) #Waiting for 4 seconds so that the page gets loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#Checking the length of each list\n",
    "print(len(brand))\n",
    "print(len(prod_desc))\n",
    "print(len(price))\n",
    "print(len(discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wrogn</td>\n",
       "      <td>Mirrored Wayfarer Sunglasses (51)</td>\n",
       "      <td>₹663</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Polarized Round Sunglasses (53)</td>\n",
       "      <td>₹799</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹758</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹509</td>\n",
       "      <td>36% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹513</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Oval Sunglasses (56)</td>\n",
       "      <td>₹284</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Mirrored, Night Vision, Riding ...</td>\n",
       "      <td>₹345</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Sports Sunglasses (Free Size)</td>\n",
       "      <td>₹771</td>\n",
       "      <td>14% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (Free Size)</td>\n",
       "      <td>₹319</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Gradient, UV Protection Cat-eye, Over-sized Su...</td>\n",
       "      <td>₹664</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand                                Product Description Price  \\\n",
       "0           Wrogn                  Mirrored Wayfarer Sunglasses (51)  ₹663   \n",
       "1   VINCENT CHASE                    Polarized Round Sunglasses (53)  ₹799   \n",
       "2        Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹758   \n",
       "3        Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...  ₹509   \n",
       "4        Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹513   \n",
       "..            ...                                                ...   ...   \n",
       "95   Silver Kartz                 UV Protection Oval Sunglasses (56)  ₹284   \n",
       "96          NuVew  UV Protection, Mirrored, Night Vision, Riding ...  ₹345   \n",
       "97       Fastrack        UV Protection Sports Sunglasses (Free Size)  ₹771   \n",
       "98     PHENOMENAL    UV Protection Clubmaster Sunglasses (Free Size)  ₹319   \n",
       "99      ROYAL SON  Gradient, UV Protection Cat-eye, Over-sized Su...  ₹664   \n",
       "\n",
       "   Discount  \n",
       "0   73% off  \n",
       "1   60% off  \n",
       "2   15% off  \n",
       "3   36% off  \n",
       "4   35% off  \n",
       "..      ...  \n",
       "95  76% off  \n",
       "96  62% off  \n",
       "97  14% off  \n",
       "98  84% off  \n",
       "99  55% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 100 sunglasses\n",
    "sunglasses=pd.DataFrame({})\n",
    "sunglasses[\"Brand\"]=brand[:100]\n",
    "sunglasses[\"Product Description\"]=prod_desc[:100]\n",
    "sunglasses[\"Price\"]=price[:100]\n",
    "sunglasses[\"Discount\"]=discount[:100]\n",
    "#printing dataframe\n",
    "sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to scrape the attributes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the required webpage\n",
    "url=\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']\").click() #Clicking on \"All reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists for scraping data\n",
    "rating=[]\n",
    "review=[]\n",
    "ful_review=[]\n",
    "urls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running for loop over 10 pages as each page consists of 10 reviews\n",
    "for i in range(10):\n",
    "    #Scraping the ratings\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\"):\n",
    "        rating.append(j.text)\n",
    "    \n",
    "    #Scraping the review summaries\n",
    "    for j in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\"):\n",
    "        review.append(j.text)\n",
    "    \n",
    "    #Scraping the full reviews\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']//div//div\"):\n",
    "        ful_review.append(j.text)\n",
    "        \n",
    "    #Scraping the urls for the next nine pages\n",
    "    url=driver.find_element_by_xpath(\"//a[@class='ge-49M']\").get_attribute('href')\n",
    "    driver.get(url)#Getting the next web page\n",
    "    time.sleep(4) #Waiting for 4 seconds so that the page gets loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#checking the length of each list\n",
    "print(len(rating))\n",
    "print(len(review))\n",
    "print(len(ful_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>It's my first time to use iOS phone and I am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Iphone is just awesome.. battery backup is ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Excellent camera, good performance, no lag. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>It’s been almost a month since I have been usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Really worth of money. i just love it. It is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings      Review Summary  \\\n",
       "0        5      Simply awesome   \n",
       "1        5           Brilliant   \n",
       "2        5           Fabulous!   \n",
       "3        5    Perfect product!   \n",
       "4        5   Worth every penny   \n",
       "..     ...                 ...   \n",
       "95       5  Highly recommended   \n",
       "96       5    Perfect product!   \n",
       "97       5      Simply awesome   \n",
       "98       5   Worth every penny   \n",
       "99       5            Terrific   \n",
       "\n",
       "                                          Full Review  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "2   This is my first iOS phone. I am very happy wi...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  It's my first time to use iOS phone and I am l...  \n",
       "96  Iphone is just awesome.. battery backup is ver...  \n",
       "97  Excellent camera, good performance, no lag. Th...  \n",
       "98  It’s been almost a month since I have been usi...  \n",
       "99  Really worth of money. i just love it. It is t...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 100 reviews\n",
    "reviews=pd.DataFrame({})\n",
    "reviews[\"Ratings\"]=rating\n",
    "reviews[\"Review Summary\"]=review\n",
    "reviews[\"Full Review\"]=ful_review\n",
    "#printing dataframe\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the required webpage\n",
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"sneakers\" in the product search bar\n",
    "driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\").send_keys(\"sneakers\") #product search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click() #clicking on the search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists for scraping data\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "price=[]\n",
    "discount=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping data from the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the brand names\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "    brand.append(i.text)\n",
    "\n",
    "#Scraping the product descriptions\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_2B099V']//a[1]\"):\n",
    "    prod_desc.append(i.text)\n",
    "\n",
    "#Scraping the prices\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "    price.append(i.text)\n",
    "\n",
    "#Scraping the discounts\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\"):\n",
    "    discount.append(i.text)\n",
    "    \n",
    "#Clicking the Next button\n",
    "driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\").click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping data from the next two pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop to run over two pages\n",
    "for i in range(2):\n",
    "    #Scraping the brand names\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "        brand.append(j.text)\n",
    "\n",
    "    #Scraping the product descriptions\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2B099V']//a[1]\"):\n",
    "        prod_desc.append(j.text)\n",
    "\n",
    "    #Scraping the prices\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "        price.append(j.text)\n",
    "\n",
    "    #Scraping the discounts\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\"):\n",
    "        discount.append(j.text)\n",
    "     \n",
    "    #clicking on the next button\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]\").click()\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#checking the length of each list\n",
    "print(len(brand))\n",
    "print(len(prod_desc))\n",
    "print(len(price))\n",
    "print(len(discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Echor</td>\n",
       "      <td>HAWK21 Sneakers For Men</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zorth</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹449</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹377</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Numenzo</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹378</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>5011-Latest Collection Stylish Casual Loafer S...</td>\n",
       "      <td>₹240</td>\n",
       "      <td>51% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>bluemaker</td>\n",
       "      <td>casual for men Sneakers For Men</td>\n",
       "      <td>₹474</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Jack Diamond</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹635</td>\n",
       "      <td>36% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>CAMPUS</td>\n",
       "      <td>EXPLORE Sneakers For Men</td>\n",
       "      <td>₹1,329</td>\n",
       "      <td>30% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>tigonis</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹331</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>India hub</td>\n",
       "      <td>Fashionable casual sneakers shoes Sneakers For...</td>\n",
       "      <td>₹404</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand                                Product Description  \\\n",
       "0                 Echor                            HAWK21 Sneakers For Men   \n",
       "1                 Zorth                                   Sneakers For Men   \n",
       "2              Magnolia                                   Sneakers For Men   \n",
       "3               Numenzo                                   Sneakers For Men   \n",
       "4   World Wear Footwear  5011-Latest Collection Stylish Casual Loafer S...   \n",
       "..                  ...                                                ...   \n",
       "95            bluemaker                    casual for men Sneakers For Men   \n",
       "96         Jack Diamond                                   Sneakers For Men   \n",
       "97               CAMPUS                           EXPLORE Sneakers For Men   \n",
       "98              tigonis                                   Sneakers For Men   \n",
       "99            India hub  Fashionable casual sneakers shoes Sneakers For...   \n",
       "\n",
       "     Price Discount  \n",
       "0     ₹599  40% off  \n",
       "1     ₹449  55% off  \n",
       "2     ₹377  62% off  \n",
       "3     ₹378  62% off  \n",
       "4     ₹240  51% off  \n",
       "..     ...      ...  \n",
       "95    ₹474  52% off  \n",
       "96    ₹635  36% off  \n",
       "97  ₹1,329  30% off  \n",
       "98    ₹331  81% off  \n",
       "99    ₹404  86% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 100 sneakers data\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers[\"Brand\"]=brand[:100]\n",
    "sneakers[\"Product Description\"]=prod_desc[:100]\n",
    "sneakers[\"Price\"]=price[:100]\n",
    "sneakers[\"Discount\"]=discount[:100]\n",
    "#printing dataframe\n",
    "sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Go to the link - https://www.myntra.com/shoes. Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”. And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the required webpage\n",
    "url=\"https://www.myntra.com/shoes\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the \"Black\" color filter radio button\n",
    "driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div\").click()\n",
    "\n",
    "time.sleep(4) #Giving few seconds for the webpage to load\n",
    "\n",
    "#checking the price filter check box\n",
    "driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists for sscraping data\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop to scrape the data from two pages\n",
    "for j in range(2):\n",
    "    #Scraping the brand names\n",
    "    for i in driver.find_elements_by_xpath(\"//h3[@class='product-brand']\"):\n",
    "        brand.append(i.text)\n",
    "    \n",
    "    #Scraping the shoe descriptions\n",
    "    for i in driver.find_elements_by_xpath(\"//h4[@class='product-product']\"):\n",
    "        description.append(i.text)\n",
    "    \n",
    "    #Scraping the prices\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='product-price']\"):\n",
    "        price.append(i.text)\n",
    "       \n",
    "    #Clicking on the next button\n",
    "    driver.find_element_by_xpath(\"//li[@class='pagination-next']//a\").click()\n",
    "    time.sleep(4) #Waiting for 4 seconds for the webpage to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#checking the length of each list\n",
    "print(len(brand))\n",
    "print(len(description))\n",
    "print(len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Shoe Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Sneakers</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men ZOOM SPAN 3 Running Shoes</td>\n",
       "      <td>Rs. 5396Rs. 7195(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Women Response SR Running</td>\n",
       "      <td>Rs. 6079Rs. 7599(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Jordan Max Aura 3 Shoes</td>\n",
       "      <td>Rs. 10295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men SPEEDREP Training Shoes</td>\n",
       "      <td>Rs. 5396Rs. 7195(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Solid Leather Sneakers</td>\n",
       "      <td>Rs. 8099Rs. 8999(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Leather Monk Shoes</td>\n",
       "      <td>Rs. 5399Rs. 5999(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Ruosh</td>\n",
       "      <td>Men Textured Derbys</td>\n",
       "      <td>Rs. 5394Rs. 8990(40% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Vanilla Moon</td>\n",
       "      <td>Women Solid Block Heels</td>\n",
       "      <td>Rs. 5990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>RAPAWALK</td>\n",
       "      <td>Standard Width Leather Oxfords</td>\n",
       "      <td>Rs. 8850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand                Shoe Description                      Price\n",
       "0           ALDO                    Men Sneakers                   Rs. 9999\n",
       "1           Nike   Men ZOOM SPAN 3 Running Shoes  Rs. 5396Rs. 7195(25% OFF)\n",
       "2         ADIDAS       Women Response SR Running  Rs. 6079Rs. 7599(20% OFF)\n",
       "3           Nike     Men Jordan Max Aura 3 Shoes                  Rs. 10295\n",
       "4           Nike     Men SPEEDREP Training Shoes  Rs. 5396Rs. 7195(25% OFF)\n",
       "..           ...                             ...                        ...\n",
       "95          Geox      Men Solid Leather Sneakers  Rs. 8099Rs. 8999(10% OFF)\n",
       "96  Hush Puppies          Men Leather Monk Shoes  Rs. 5399Rs. 5999(10% OFF)\n",
       "97         Ruosh             Men Textured Derbys  Rs. 5394Rs. 8990(40% OFF)\n",
       "98  Vanilla Moon         Women Solid Block Heels                   Rs. 5990\n",
       "99      RAPAWALK  Standard Width Leather Oxfords                   Rs. 8850\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data for shoes\n",
    "shoes=pd.DataFrame({})\n",
    "shoes[\"Brand\"]=brand\n",
    "shoes[\"Shoe Description\"]=description\n",
    "shoes[\"Price\"]=price\n",
    "#printing dataframe\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon.Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”. After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the required webpage\n",
    "url=' https://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"laptop\" in the product search bar\n",
    "driver.find_element_by_id(\"twotabsearchtextbox\").send_keys(\"Laptop\") #product search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_id(\"nav-search-submit-button\").click() #clicking on the search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists for scraping data\n",
    "title=[]\n",
    "ratings=[]\n",
    "price=[]\n",
    "urls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the checkbox for \"Intel Core i7\"\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[17]/li[26]/span/a/span\").click()\n",
    "\n",
    "time.sleep(4) #Giving few seconds for the webpage to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once webpage is loaded, clicking the checkbox for \"Intel Core i9\"\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[17]/li[28]/span/a/span\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the titles of laptop\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\"):\n",
    "    title.append(i.text)\n",
    "\n",
    "#Scraping the urls for each laptop\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\"):\n",
    "    urls.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running for loop over first 10 urls to scrape price and ratings\n",
    "for i in urls[:10]:\n",
    "    driver.get(i) #getting each url page\n",
    "    try:\n",
    "        #Scraping the price\n",
    "        p=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-price priceBlockBuyingPriceString']\")\n",
    "        price.append(p.text)\n",
    "    except:\n",
    "        price.append(\"-\") #If price is not present, then appending it with \"-\"\n",
    "        \n",
    "    try:\n",
    "        #Scraping the ratings\n",
    "        r=driver.find_element_by_xpath(\"//span[@class='a-size-base a-nowrap']//span\")\n",
    "        ratings.append(r.text)\n",
    "    except:\n",
    "        ratings.append(\"-\") #If ratings is not present, then appending it with \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUS TUF Dash F15 (2021), 15.6\" (39.62 cms) FH...</td>\n",
       "      <td>₹1,07,990.00</td>\n",
       "      <td>4.5 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP Envy 11th Gen Core i7 Processor 13.3-inch (...</td>\n",
       "      <td>₹1,09,990.00</td>\n",
       "      <td>4 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP Pavilion (2021) Thin &amp; Light 11th Gen Core ...</td>\n",
       "      <td>₹84,990.00</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSI GF65 Thin, Intel i7-10750H, 15.6\" FHD (39....</td>\n",
       "      <td>₹81,990.00</td>\n",
       "      <td>3.8 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Life Digital Laptop 15.6-inch (39.62 cms) (Int...</td>\n",
       "      <td>₹34,990.00</td>\n",
       "      <td>3.9 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP Pavilion Gaming 10th Gen Intel Core i7 Proc...</td>\n",
       "      <td>₹86,990.00</td>\n",
       "      <td>4.2 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP Pavilion x360 (2021) 14\" (35.56cms) FHD Tou...</td>\n",
       "      <td>₹81,999.00</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Life Digital Laptop 15.6-inch (39.62 cms) (Int...</td>\n",
       "      <td>₹39,990.00</td>\n",
       "      <td>3.2 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OMEN 17 Gaming Laptop, NVIDIA GeForce RTX 2070...</td>\n",
       "      <td>-</td>\n",
       "      <td>4.5 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...</td>\n",
       "      <td>₹88,990.00</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title         Price  \\\n",
       "0  ASUS TUF Dash F15 (2021), 15.6\" (39.62 cms) FH...  ₹1,07,990.00   \n",
       "1  HP Envy 11th Gen Core i7 Processor 13.3-inch (...  ₹1,09,990.00   \n",
       "2  HP Pavilion (2021) Thin & Light 11th Gen Core ...    ₹84,990.00   \n",
       "3  MSI GF65 Thin, Intel i7-10750H, 15.6\" FHD (39....    ₹81,990.00   \n",
       "4  Life Digital Laptop 15.6-inch (39.62 cms) (Int...    ₹34,990.00   \n",
       "5  HP Pavilion Gaming 10th Gen Intel Core i7 Proc...    ₹86,990.00   \n",
       "6  HP Pavilion x360 (2021) 14\" (35.56cms) FHD Tou...    ₹81,999.00   \n",
       "7  Life Digital Laptop 15.6-inch (39.62 cms) (Int...    ₹39,990.00   \n",
       "8  OMEN 17 Gaming Laptop, NVIDIA GeForce RTX 2070...             -   \n",
       "9  Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...    ₹88,990.00   \n",
       "\n",
       "        Ratings  \n",
       "0  4.5 out of 5  \n",
       "1    4 out of 5  \n",
       "2  4.4 out of 5  \n",
       "3  3.8 out of 5  \n",
       "4  3.9 out of 5  \n",
       "5  4.2 out of 5  \n",
       "6  4.1 out of 5  \n",
       "7  3.2 out of 5  \n",
       "8  4.5 out of 5  \n",
       "9  4.1 out of 5  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the dataframe from the scraped data and taking only first 10 laptops\n",
    "laptops=pd.DataFrame({})\n",
    "laptops[\"Title\"]=title[:10]\n",
    "laptops[\"Price\"]=price\n",
    "laptops[\"Ratings\"]=ratings\n",
    "#printing dataframe\n",
    "laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
